{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tgrLGOCV0wT"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2mTZv_7g0-i"
   },
   "outputs": [],
   "source": [
    "!pip -q install configparser langchain google-generativeai chromadb > /dev/null\n",
    "!pip -q install transformers huggingface_hub sentence_transformers > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbl2u6sG5GxK"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9O9ctkug5PR"
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-blilkQx7yY"
   },
   "outputs": [],
   "source": [
    "#Then convert the transformers pipeline into fact_llm \n",
    "\n",
    "fact_llm = GooglePalm(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw0ULJdKYc6d"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt_open = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "open_chain = LLMChain(prompt=prompt_open,llm = fact_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq6do1GvYc2W",
    "outputId": "97e37191-dbbe-4b88-c07e-b966a2a30f02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Justin Beiber was born in 1994, so the Super Bowl was in 2015. The team that won the Super Bowl in 2015 was the New England Patriots.\n"
     ]
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "print(open_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE1P3uZ0YcdE"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "recurSplitter = RecursiveCharacterTextSplitter(chunk_size=100,\n",
    "                                               chunk_overlap=20,\n",
    "                                               length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc2UejsER-up"
   },
   "outputs": [],
   "source": [
    "with open('/content/linux_play.txt') as lin:\n",
    "  txt_lin = lin.read()\n",
    "\n",
    "linux_docs = recurSplitter.create_documents([txt_lin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld9dOjyiR-sU"
   },
   "outputs": [],
   "source": [
    "with open('/content/space_shortened.csv') as spc:\n",
    "  txt_spc = spc.read()\n",
    "\n",
    "space_docs = recurSplitter.create_documents([txt_spc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8kMbvyDR-nt"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings \n",
    "hfEmbed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#The all-MiniLM has 384 vector elements. I suspect it may or may not work \n",
    "#with paLM. The alternate will be to work with instructor-xl. Uncomment \n",
    "#below code and execute\n",
    "\n",
    "#hfEmbed = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Weno1_dzR-k1",
    "outputId": "25bd7c33-733e-44a2-d445-4df2313bc8e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: space_db\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "space_chroma = Chroma.from_documents(documents=space_docs,\n",
    "                                     embeddings=hfEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QM4x9biDR-dE",
    "outputId": "0de16a7b-1474-4eb6-fdfc-6d25af172062"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: lin_db\n",
      "WARNING:chromadb.api.models.Collection:No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "lin_chroma = Chroma.from_documents(documents=linux_docs,\n",
    "                                     embeddings=hfEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T73D7bKe4FJ3"
   },
   "outputs": [],
   "source": [
    "# Starting the custom tool route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESDPHjul3Gi5"
   },
   "outputs": [],
   "source": [
    "lin_retriever =RetrievalQA.from_chain_type(llm=fact_llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=lin_chroma.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaEXq4hs74NR"
   },
   "outputs": [],
   "source": [
    "lin_retriever(\"What this documents are about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g52G_gi43ToT"
   },
   "outputs": [],
   "source": [
    "space_retriever =RetrievalQA.from_chain_type(llm=fact_llm, \n",
    "                                           chain_type=\"stuff\", \n",
    "                                           retriever=space_chroma.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1E9JVHD79Zi"
   },
   "outputs": [],
   "source": [
    "space_retriever(\"how many passengers are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm6ArvS4WyAX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
